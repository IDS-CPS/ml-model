{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../dataset/swat-minimized.csv\", delimiter=\";\", decimal=\",\")\n",
    "# df = df[16000:]\n",
    "df = df.drop(\"Normal/Attack\", axis=1)\n",
    "df = df.drop(\"Timestamp\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dropped = [\"AIT201\", \"AIT202\", \"AIT203\", \"P201\", \"AIT401\",\n",
    "\"AIT402\", \"AIT501\", \"AIT502\", 'AIT503', \"AIT504\", \"FIT503\", \"FIT504\",\n",
    "\"PIT501\", \"PIT502\", \"PIT503\"]\n",
    "\n",
    "df = df.drop(columns=features_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "train_df = df[0:int(n*0.8)]\n",
    "test_df = df[int(n*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features used:  Index(['FIT101', 'LIT101', 'MV101', 'P101', 'P102', 'FIT201', 'MV201', 'P202',\n",
      "       'P203', 'P204', 'P205', 'P206', 'DPIT301', 'FIT301', 'LIT301', 'MV301',\n",
      "       'MV302', 'MV303', 'MV304', 'P301', 'P302', 'FIT401', 'LIT401', 'P401',\n",
      "       'P402', 'P403', 'P404', 'UV401', 'FIT501', 'FIT502', 'P501', 'P502',\n",
      "       'FIT601', 'P601', 'P602', 'P603'],\n",
      "      dtype='object')\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(\"Features used: \", df.columns)\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "\t\"\"\"\n",
    "\tWe create time windows to create X and y features.\n",
    "\tFor example, if we choose a window of 30, we will create a dataset formed by 30 points as X\n",
    "\t\"\"\"\n",
    "\tdataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "\tdataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "\tdataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "\tdataset = dataset.shuffle(shuffle_buffer)\n",
    "\tdataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "\tdataset = dataset.batch(batch_size).prefetch(1)\n",
    "\treturn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = windowed_dataset(train_df, 5, 32, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(799, 36)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_uncompiled_model():\n",
    "  # define a sequential model\n",
    "  model = tf.keras.models.Sequential([ \n",
    "      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1024, return_sequences=True, input_shape=(-1, 36))),\n",
    "      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True)),\n",
    "      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),\n",
    "      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "      tf.keras.layers.Dense(36),\n",
    "  ]) \n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    tf.random.set_seed(51)\n",
    "  \n",
    "    model = create_uncompiled_model()\n",
    "  \n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(), \n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  metrics=[\"mae\"])  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 36s 632ms/step - loss: 1371.5447 - mae: 11.1690\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "# we train for 20 epochs with and assign the callback\n",
    "history = model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forecast(model, series, window_size):\n",
    "    \"\"\"This function converts the input series into a dataset with time windows for forecasting\"\"\"\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "    ds = ds.batch(32).prefetch(1)\n",
    "    forecast = model.predict(ds)\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000000e+00 1.243135e+02 1.000000e+00 1.000000e+00 1.000000e+00\n",
      " 0.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00\n",
      " 1.000000e+00 1.000000e+00 2.560983e+00 2.562210e-04 1.385061e+02\n",
      " 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00\n",
      " 1.000000e+00 0.000000e+00 1.338503e+02 1.000000e+00 1.000000e+00\n",
      " 1.000000e+00 1.000000e+00 1.000000e+00 1.538067e-03 1.408992e-03\n",
      " 1.000000e+00 1.000000e+00 2.563040e-04 1.000000e+00 1.000000e+00\n",
      " 1.000000e+00]\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "[[ 0.3976486   6.200303    0.9766389   0.72598594  0.5523159   0.11366516\n",
      "   0.8682514   0.54315764  0.6672812   0.77033067  0.5518025   0.7041555\n",
      "   1.8040266  -0.04833576  5.7031274   0.62101424  0.63455474  0.878651\n",
      "   0.8848462   0.6672926   0.5831128   0.12936716  6.4631824   0.8976166\n",
      "   0.3642763   0.7275253   0.53227836  0.9897749   0.04938983 -0.07585277\n",
      "   0.8468361   0.73668593  0.03170721  0.66168594  0.8987491   0.93287766]]\n"
     ]
    }
   ],
   "source": [
    "test = df.to_numpy()\n",
    "print(test[0])\n",
    "pred = model.predict(test[0][np.newaxis][np.newaxis])\n",
    "\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
